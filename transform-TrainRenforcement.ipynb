{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be options chain day1 - end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /venv/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: duckdb in /venv/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: tqdm in /venv/lib/python3.10/site-packages (4.66.5)\n",
      "Collecting minio\n",
      "  Downloading minio-7.2.8-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: certifi in /venv/lib/python3.10/site-packages (from minio) (2024.8.30)\n",
      "Requirement already satisfied: urllib3 in /venv/lib/python3.10/site-packages (from minio) (2.2.3)\n",
      "Requirement already satisfied: argon2-cffi in /venv/lib/python3.10/site-packages (from minio) (23.1.0)\n",
      "Collecting pycryptodome (from minio)\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: typing-extensions in /venv/lib/python3.10/site-packages (from minio) (4.12.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /venv/lib/python3.10/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /venv/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /venv/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.22)\n",
      "Downloading minio-7.2.8-py3-none-any.whl (93 kB)\n",
      "Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycryptodome, minio\n",
      "Successfully installed minio-7.2.8 pycryptodome-3.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install duckdb\n",
    "!pip install tqdm\n",
    "!pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from IPython.display import clear_output\n",
    "import joblib\n",
    "import random\n",
    "import hashlib\n",
    "from src.wgangp.utils import Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH     = './data/OptionsEOD.csv/'\n",
    "#create @ part 1\n",
    "PARQUET_PATH = './data/OptionsEOD.parquet'\n",
    "#create @ part 2\n",
    "PARQUET_STG_PATH = './data/OptionsEOD_STG.parquet'\n",
    "SCALER_COL  = [ 'UNDERLYING_LAST','STRIKE','STRIKE_DISTANCE','INTRINSIC_VALUE','DTE','C_VEGA','P_VEGA',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK', 'P_VOLUME' ]\n",
    "#create @ part 3\n",
    "OTHER_COL = ['OPTIONS_ID','QUOTE_UNIXTIME', 'EXPIRE_UNIX']\n",
    "\n",
    "QUOTE_COL = ['C_BID',\t'C_ASK',  'P_BID',\t'P_ASK']\n",
    "VEGA_COL =  [\"C_VEGA\",\"P_VEGA\"] \n",
    "VOLUME_COL =  [\"TOTAL_VOLUME\",\"C_VOLUME\",\"P_VOLUME\"] \n",
    "\n",
    "SCALER_QUOTE_COL_INDEX = [i for i,v in enumerate(SCALER_COL) if v in QUOTE_COL]\n",
    "SCALER_VEGA_COL_INDEX = [i for i,v in enumerate(SCALER_COL) if v in VEGA_COL]\n",
    "SCALER_VOLUME_COL_INDEX = [i for i,v in enumerate(SCALER_COL) if v in VOLUME_COL]\n",
    "SCALER_OTHER_COL_INDEX = [i for i,v in enumerate(SCALER_COL) if v not in QUOTE_COL+VEGA_COL+VOLUME_COL ]\n",
    "\n",
    "\n",
    "UNIQUE_KEYS = ['SYMBOL','EXPIRE_DATE']\n",
    "H5_PATH = './data/OptTrainRenforcement/'\n",
    "START = True#True#'2010-09'#True\n",
    "BUCKET = 'tensorflow'\n",
    "\n",
    "max_option_len = 16\n",
    "#==== Create options_qoute\n",
    "options_qoute = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "EOD_CSV = pd.read_csv(CSV_PATH+\"qqq/qqq_eod_201201.txt\", engine='pyarrow')\n",
    "EOD_CSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==================================Create OptionsEOD.parquet part 1========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part I\n",
    "#TransformData : \n",
    "#-each partition from EXPIRE_DATE \n",
    "#-csv too parquet\n",
    "#-col. rename \n",
    "def TransformDataI():\n",
    "    #scaler = MinMaxScaler()\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    schema = None\n",
    "    pqwriter = None\n",
    "    for d in os.listdir(CSV_PATH):\n",
    "        for f in os.listdir(CSV_PATH+f\"{d}/\"):\n",
    "            if f.endswith(\".txt\"):\n",
    "                ## load\n",
    "                print( f\"[LOAD] : {CSV_PATH}{d}/{f}        \",end='\\r')\n",
    "                EOD_CSV = pd.read_csv(CSV_PATH+f\"{d}/\"+f, engine='pyarrow')\n",
    "                    \n",
    "                ## rename col.\n",
    "                for c in EOD_CSV.columns:\n",
    "                    EOD_CSV = EOD_CSV.rename( columns={ c:c.strip().replace(']','').replace('[','') } )\n",
    "                \n",
    "                ## add symbol \n",
    "                EOD_CSV['SYMBOL'] = d.upper()\n",
    "                ## add INTRINSIC_VALUE\n",
    "                EOD_CSV['INTRINSIC_VALUE'] = EOD_CSV['UNDERLYING_LAST'] - EOD_CSV['STRIKE']\n",
    "                \n",
    "                ## fillnafillna\n",
    "                EOD_CSV['P_VOLUME'] = EOD_CSV['P_VOLUME'].fillna(0)\n",
    "                EOD_CSV['C_VOLUME'] = EOD_CSV['C_VOLUME'].fillna(0)\n",
    "                EOD_CSV.dropna(subset=[SCALER_COL])\n",
    "            \n",
    "                \n",
    "                # date columns convert to datetime\n",
    "                for c in [\"QUOTE_READTIME\",\"QUOTE_DATE\",\"EXPIRE_DATE\"]:\n",
    "                    EOD_CSV[c] = pd.to_datetime(EOD_CSV[c])\n",
    "                \n",
    "                #clean float data\n",
    "                for c in ['INTRINSIC_VALUE','C_DELTA','C_GAMMA','C_VEGA','C_THETA','C_RHO','C_IV','C_VOLUME','C_LAST','C_BID','C_ASK','STRIKE','P_BID','P_ASK','P_LAST','P_DELTA','P_GAMMA','P_VEGA','P_THETA','P_RHO','P_IV','P_VOLUME','STRIKE_DISTANCE','STRIKE_DISTANCE_PCT']:\n",
    "                    if EOD_CSV[c].dtype not in ( 'float32','float64'):\n",
    "                        EOD_CSV[c] = EOD_CSV[c].apply(lambda x: x.strip())\n",
    "                        EOD_CSV[c] = EOD_CSV[c].replace('', np.nan).fillna(np.nan)\n",
    "                        EOD_CSV[c] = EOD_CSV[c].astype('float64')\n",
    "                    if EOD_CSV[c].dtype == 'float32':\n",
    "                        EOD_CSV[c] = EOD_CSV[c].astype('float64')\n",
    "                        \n",
    "                # REMAIN_DAYS(int) =>  use DTE col.\n",
    "                #partition with QUOTE_DATE\n",
    "                EOD_CSV['PartitionDate'] = EOD_CSV['QUOTE_DATE'].dt.strftime('%Y-%m')\n",
    "                EOD_CSV.sort_values(['QUOTE_DATE','EXPIRE_DATE','SYMBOL','STRIKE'],ascending =False ) \n",
    "\n",
    "                #scaler(Normalization_\n",
    "                #scaler.partial_fit(EOD_CSV[SCALER_COL])\n",
    "\n",
    "                # save\n",
    "                if os.path.exists(PARQUET_PATH):\n",
    "                  EOD_CSV.to_parquet(PARQUET_PATH, engine='fastparquet', append=True, partition_cols=['PartitionDate'], index=False )\n",
    "                else:\n",
    "                  EOD_CSV.to_parquet(PARQUET_PATH, engine='fastparquet' , partition_cols=['PartitionDate'], index=False  )\n",
    "                    \n",
    "    # joblib.dump(scaler, SCALER_PATH )\n",
    "    # if pqwriter:\n",
    "    #     pqwriter.close()\n",
    "    # print( f\"[DONE]                                                       \",end='\\r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-RunCleanData\n",
    "#TransformDataI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==================================Create OptTrainData STG part 2========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()  # By default, it loads the .env file from the current directory\n",
    "\n",
    "# Example: Access environment variables\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'host.containers.internal:9000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('MINIO_HOST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcb2c1a3bf0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect(database=':memory:')\n",
    "con.execute(f\"\"\"\n",
    "CREATE SECRET secretMinioProd (\n",
    "    TYPE S3,\n",
    "    KEY_ID '{os.getenv('MINIO_USER')}',\n",
    "    SECRET '{os.getenv('MINIO_SECRET')}',\n",
    "    ENDPOINT '{os.getenv('MINIO_HOST')}',\n",
    "    USE_SSL 'false',\n",
    "    URL_STYLE 'path'\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.86s/it]\n"
     ]
    }
   ],
   "source": [
    "##### Part II \n",
    "from tqdm import tqdm\n",
    "\n",
    "#START = '2012-02'\n",
    "#TransformData : \n",
    "# - read each partitions \n",
    "# - Normalization if not have scaler.gz file\n",
    "# ###===== load Scaler\n",
    "# SCALER = Scaler(\n",
    "#     [v for v in SCALER_COL if v not in QUOTE_COL+VEGA_COL+VOLUME_COL ] + ['QUOTE','VEGA','VOLUME']\n",
    "#     , create=True\n",
    "#     , path=\"./data/scaler/\")\n",
    "\n",
    "if START == True:\n",
    "    pass\n",
    "###===== local functions\n",
    "def strikeZero(df,v,qv,num_rm):   \n",
    "    # First, filter based on QUOTE_DATE, SYMBOL, and EXPIRE_DATE\n",
    "    filtered_arr = df[(df['QUOTE_DATE'] == qv['QUOTE_DATE']) &\n",
    "                     (df['SYMBOL'] == v['SYMBOL']) &\n",
    "                     (df['EXPIRE_DATE'] == v['EXPIRE_DATE'])]['STRIKE'].values\n",
    "    \n",
    "    if num_rm != 0 :\n",
    "        df.loc[ (df['QUOTE_DATE'] == qv['QUOTE_DATE']) \n",
    "                & (df['SYMBOL'] == v['SYMBOL']) \n",
    "                & (df['EXPIRE_DATE'] == v['EXPIRE_DATE'])\n",
    "                & (  (df['STRIKE'].isin(filtered_arr[:num_rm])  )\n",
    "                   | (df['STRIKE'].isin(filtered_arr[-num_rm:])   )\n",
    "                  ) \n",
    "            , SCALER_COL    \n",
    "        ] = 0\n",
    "    \n",
    "def hash_str(S):\n",
    "    return int(hashlib.md5(S.encode('utf-8')).hexdigest(),16)\n",
    "    \n",
    "#========================================= main model =========================================\n",
    "keys = None#df[unique_keys].sort_values(by=unique_keys).drop_duplicates()\n",
    "\n",
    "#==== Get PartitionDate\n",
    "PartitionDate = [ d[-7:] for d in  os.listdir(PARQUET_PATH) if 'PartitionDate' in d]\n",
    "#PartitionDate = ['2011-12','2022-05'] # debug\n",
    "debug = None\n",
    "for i,partdate in  enumerate(tqdm(PartitionDate[:1]) ) :  \n",
    "    START = True if START == partdate else START\n",
    "    if START == True :\n",
    "        df = pd.read_parquet(PARQUET_PATH,engine='pyarrow'\n",
    "                                     , filters=[('PartitionDate', '=', partdate)]\n",
    "                                    )\n",
    "        df['PartitionDate'] = df['PartitionDate'].astype('object') \n",
    "        #add col options_id\n",
    "        df['P_VOLUME'] = df['P_VOLUME'].fillna(0)\n",
    "        df['C_VOLUME'] = df['C_VOLUME'].fillna(0)\n",
    "        df[df == 0] = 0\n",
    "        df = df.dropna(subset=[c for c in SCALER_COL if c not in ('ID','QUOTE_DATE', 'EXPIRE_DATE', 'TOTAL_VOLUME') ])\n",
    "        ####################################################\n",
    "        keys_item = df[UNIQUE_KEYS].sort_values(by=UNIQUE_KEYS).drop_duplicates()[:]\n",
    "        \n",
    "        #loop each UNIQUE_KEYS keys \n",
    "        for j,v in keys_item.iterrows():\n",
    "            msg = {}\n",
    "            \n",
    "            msg['symbo'] = v['SYMBOL']\n",
    "            msg['exdate'] = v['EXPIRE_DATE']\n",
    "            \n",
    "            df_Item=df[(df['SYMBOL'] == v['SYMBOL']) & (df['EXPIRE_DATE'] == v['EXPIRE_DATE']) ]\n",
    "            keys_quote = df_Item[['QUOTE_DATE']].sort_values(by=['QUOTE_DATE']).drop_duplicates()[:]\n",
    "            DATA = pd.DataFrame([],columns=OTHER_COL+SCALER_COL)\n",
    "            for k,qv in keys_quote.iterrows():\n",
    "                msg['qd_ate'] = qv['QUOTE_DATE']\n",
    "                df_filter =df_Item[(df_Item['QUOTE_DATE'] == qv['QUOTE_DATE']) ]\n",
    "                #df_filter['OPTIONS_ID'] = df_filter['OPTIONS_ID'].astype('int') \n",
    "                # df_filter['EXPIRE_DATE'] = df['EXPIRE_DATE'].astype('S10') \n",
    "                # df_filter['OPTIONS_ID'] = df['OPTIONS_ID'].astype('S10') \n",
    "                msg['start_rows'] = len(df_filter) \n",
    "\n",
    "                #break loops\n",
    "                if len(df_filter) < 5 : \n",
    "                    print(f\"\"\"{msg['qd_ate']}|{msg['symbo']}|{msg['exdate']} : {msg['start_rows']}->break (less rows)\"\"\")\n",
    "                    break \n",
    "\n",
    "                qoute = \"\".join(v[ ['SYMBOL','EXPIRE_DATE'] ].apply(str).values)\n",
    "                #add new qoute\n",
    "                if qoute not in [*options_qoute.keys()]:\n",
    "                    options_qoute[qoute] = {}\n",
    "                    options_qoute[qoute]['start_price'] = df_filter['UNDERLYING_LAST'].values[0]\n",
    "                    options_qoute[qoute]['strike'] = df_filter[ df_filter['INTRINSIC_VALUE'].abs().isin(df_filter['INTRINSIC_VALUE'].abs().sort_values()[:max_option_len]) ]['STRIKE'].values\n",
    "                    options_qoute[qoute]['exp'] = df_filter['EXPIRE_DATE'].values[0]\n",
    "                    #check diff UNDERLYING_LAST\n",
    "                    if df_filter['UNDERLYING_LAST'].values[0] != round(np.average(df_filter['UNDERLYING_LAST']),4):\n",
    "                        print('[ERROR] : set UNDERLYING_LAST ',qoute )\n",
    "                #rm index max : max_option_len\n",
    "                #may be initstrike out of scope\n",
    "                rm_strike_index = df_filter[ ~df_filter['STRIKE'].isin(options_qoute[qoute]['strike']) ].index\n",
    "                df_filter = df_filter.drop(rm_strike_index)\n",
    "    \n",
    "                #break loops\n",
    "                if len(df_filter) < 5 :  \n",
    "                    print(f\"\"\"{msg['qd_ate']}|{msg['symbo']}|{msg['exdate']} : {msg['start_rows']}->break (rm_strike_index)\"\"\")\n",
    "                    break\n",
    "                    \n",
    "                # Generate zero strike \n",
    "                # Generate a random float between 0.01 and 1\n",
    "                \n",
    "                # dte = df_filter[ df_filter['DTE'] != 0]['DTE'].mean()\n",
    "                # df_filter['DTE'] = dte\n",
    "                \n",
    "                strikeZero(df_filter,v,qv,3 )\n",
    "                df_filter['OPTIONS_ID'] = int( f'{i}{j}{k}' )\n",
    "                # #++++++++++++++++++++++++++++++++++++++++++++++\n",
    "                # print( \"A\")\n",
    "                # print( len(df_filter) )\n",
    "                # print( nom_rm_rows )\n",
    "                # print( df_filter['STRIKE_DISTANCE'] )\n",
    "                # break\n",
    "                # #++++++++++++++++++++++++++++++++++++++++++++++\n",
    "                \n",
    "                #====filed rows\n",
    "                z = 0\n",
    "                while len(df_filter) != max_option_len :\n",
    "                    z += 1\n",
    "                    if z > 50: raise \"error\"\n",
    "                    Even= -1\n",
    "                    if (len(df_filter) % 2) == 0:\n",
    "                        Even = 0\n",
    "                    if len(df_filter) < max_option_len :\n",
    "                        zero_row = pd.DataFrame([[0]*len(df_filter.columns)], columns=df_filter.columns)\n",
    "                        if Even==0 :\n",
    "                            df_filter = pd.concat([df_filter, zero_row], ignore_index=True)\n",
    "                        else:\n",
    "                            df_filter = pd.concat([zero_row, df_filter], ignore_index=True)\n",
    "                    elif len(df_filter) > max_option_len :\n",
    "                         df_filter = df_filter.drop(df_filter.index[Even])\n",
    "                \n",
    "                #add cal columns.      \n",
    "                msg['end_rows']= len(df_filter)\n",
    "                if len(df_filter) == max_option_len and np.sum( df_filter[OTHER_COL+SCALER_COL].isna().values ) == 0 :\n",
    "                    #DATA = np.vstack((DATA ,[df_filter[OTHER_COL+SCALER_COL]]))   \n",
    "                    df_filter_reindexed = df_filter.reindex(columns=OTHER_COL+SCALER_COL)\n",
    "                    df_filter_reindexed['QUOTE_INDEX'] = range(1, len(df_filter_reindexed) + 1)\n",
    "                    if len(DATA) :\n",
    "                        DATA = pd.concat([DATA, df_filter_reindexed], ignore_index=True)\n",
    "                    else:\n",
    "                        DATA = df_filter_reindexed\n",
    "\n",
    "                #if len(DATA) :\n",
    "                    # #========================== partial_fit ================================#\n",
    "                    # for col_i in SCALER_OTHER_COL_INDEX:\n",
    "                    #     SCALER()[SCALER_COL[col_i]].partial_fit( DATA.reshape(-1, len(SCALER_COL))[:,[col_i]] )\n",
    "                    # SCALER()['QUOTE'].partial_fit( DATA.reshape(-1, len(SCALER_COL))[:,SCALER_QUOTE_COL_INDEX].reshape(-1, 1) )\n",
    "                    # SCALER()['VEGA'].partial_fit( DATA.reshape(-1, len(SCALER_COL))[:,SCALER_VEGA_COL_INDEX].reshape(-1, 1) )\n",
    "                    # SCALER()['VOLUME'].partial_fit( DATA.reshape(-1, len(SCALER_COL))[:,SCALER_VOLUME_COL_INDEX].reshape(-1, 1) )\n",
    "                    # SCALER.save()\n",
    "                    \n",
    "            # #=== clear expire options_qoute\n",
    "            # for qi in list(options_qoute.keys()):\n",
    "            #     if options_qoute[qi]['exp'] < quote:\n",
    "            #         options_qoute.pop(qi)\n",
    "            \n",
    "            #========================== SAVE DATA End for k============================#\n",
    "            # #=== save H5\n",
    "            # if not os.path.exists(H5_PATH):\n",
    "            #     os.makedirs(H5_PATH)\n",
    "            # with h5py.File(H5_PATH+f\"{partdate}.h5\", 'w') as f:\n",
    "            #     dset = f.create_dataset(f'{i}{j}{k}', data=DATA, chunks=True , compression='gzip')\n",
    "            # #=== save parquet minio\n",
    "            con.register('df_table', DATA)\n",
    "            parquet_file_path = f\"s3://tensorflow/renforcement/rawdata/{partdate}/{v['SYMBOL']}_{str(v['EXPIRE_DATE'])[:10]}.parquet\"\n",
    "            con.execute(f\"COPY df_table TO '{parquet_file_path}' (FORMAT PARQUET);\")\n",
    "            \n",
    "            # #=== save parquet\n",
    "            # if os.path.exists(PARQUET_STG_PATH):\n",
    "            #   df.to_parquet(PARQUET_STG_PATH, engine='fastparquet', append=True, partition_cols=['PartitionDate'], index=False )\n",
    "            # else:\n",
    "            #   df.to_parquet(PARQUET_STG_PATH, engine='fastparquet' , partition_cols=['PartitionDate'], index=False  )   \n",
    "            debug = f\"[Last Processing] {partdate}, {round(((i+1)/len(PartitionDate))*100,2)}%       \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "================= duckdb loaddata ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "# Initialize MinIO client\n",
    "minio_client = Minio(\n",
    "    os.getenv('MINIO_HOST'),  # Change to your MinIO server address\n",
    "    access_key=os.getenv('MINIO_USER'),\n",
    "    secret_key=os.getenv('MINIO_SECRET'),\n",
    "    secure=False  # Use False if not using HTTPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_list = []\n",
    "for _dir in minio_client.list_objects(BUCKET, prefix='renforcement/rawdata/'):\n",
    "    for file in minio_client.list_objects(BUCKET, prefix=_dir.object_name):\n",
    "        object_list.append(file.object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'renforcement/rawdata/2012-01/QQQ_2012-01-06.parquet'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata=con.execute(f\"\"\"\n",
    "SELECT * FROM read_parquet('s3://tensorflow/{object_list[0]}', filename = true);\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPTIONS_ID</th>\n",
       "      <th>QUOTE_UNIXTIME</th>\n",
       "      <th>EXPIRE_UNIX</th>\n",
       "      <th>UNDERLYING_LAST</th>\n",
       "      <th>STRIKE</th>\n",
       "      <th>STRIKE_DISTANCE</th>\n",
       "      <th>INTRINSIC_VALUE</th>\n",
       "      <th>DTE</th>\n",
       "      <th>C_VEGA</th>\n",
       "      <th>P_VEGA</th>\n",
       "      <th>C_BID</th>\n",
       "      <th>C_ASK</th>\n",
       "      <th>C_VOLUME</th>\n",
       "      <th>P_BID</th>\n",
       "      <th>P_ASK</th>\n",
       "      <th>P_VOLUME</th>\n",
       "      <th>QUOTE_INDEX</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1325624400</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1325624400</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1325624400</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1325624400</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>56.90</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.90</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00159</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1325624400</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>56.90</td>\n",
       "      <td>53.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00152</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.92</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>587.0</td>\n",
       "      <td>5</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1688</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>57.78</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00358</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.29</td>\n",
       "      <td>32.0</td>\n",
       "      <td>12</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1688</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>57.78</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-3.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>0.00299</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1688</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1688</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1688</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>s3://tensorflow/renforcement/rawdata/2012-01/Q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    OPTIONS_ID  QUOTE_UNIXTIME  EXPIRE_UNIX  UNDERLYING_LAST  STRIKE  \\\n",
       "0            0      1325624400   1325883600             0.00     0.0   \n",
       "1            0      1325624400   1325883600             0.00     0.0   \n",
       "2            0      1325624400   1325883600             0.00     0.0   \n",
       "3            0      1325624400   1325883600            56.90    52.0   \n",
       "4            0      1325624400   1325883600            56.90    53.0   \n",
       "..         ...             ...          ...              ...     ...   \n",
       "59        1688      1325883600   1325883600            57.78    60.0   \n",
       "60        1688      1325883600   1325883600            57.78    61.0   \n",
       "61        1688      1325883600   1325883600             0.00     0.0   \n",
       "62        1688      1325883600   1325883600             0.00     0.0   \n",
       "63        1688      1325883600   1325883600             0.00     0.0   \n",
       "\n",
       "    STRIKE_DISTANCE  INTRINSIC_VALUE  DTE   C_VEGA   P_VEGA  C_BID  C_ASK  \\\n",
       "0               0.0             0.00  0.0  0.00000  0.00000   0.00   0.00   \n",
       "1               0.0             0.00  0.0  0.00000  0.00000   0.00   0.00   \n",
       "2               0.0             0.00  0.0  0.00000  0.00000   0.00   0.00   \n",
       "3               4.9             4.90  3.0  0.00000  0.00159   4.84   4.92   \n",
       "4               3.9             3.90  3.0  0.00000  0.00152   3.85   3.92   \n",
       "..              ...              ...  ...      ...      ...    ...    ...   \n",
       "59              2.2            -2.22  0.0  0.00049  0.00358   0.00   0.01   \n",
       "60              3.2            -3.22  0.0  0.00054  0.00299   0.00   0.01   \n",
       "61              0.0             0.00  0.0  0.00000  0.00000   0.00   0.00   \n",
       "62              0.0             0.00  0.0  0.00000  0.00000   0.00   0.00   \n",
       "63              0.0             0.00  0.0  0.00000  0.00000   0.00   0.00   \n",
       "\n",
       "    C_VOLUME  P_BID  P_ASK  P_VOLUME  QUOTE_INDEX  \\\n",
       "0        0.0    0.0   0.00       0.0            1   \n",
       "1        0.0    0.0   0.00       0.0            2   \n",
       "2        0.0    0.0   0.00       0.0            3   \n",
       "3        0.0    0.0   0.01     100.0            4   \n",
       "4       22.0    0.0   0.01     587.0            5   \n",
       "..       ...    ...    ...       ...          ...   \n",
       "59       0.0    2.2   2.29      32.0           12   \n",
       "60       0.0    3.2   3.29       0.0           13   \n",
       "61       0.0    0.0   0.00       0.0           14   \n",
       "62       0.0    0.0   0.00       0.0           15   \n",
       "63       0.0    0.0   0.00       0.0           16   \n",
       "\n",
       "                                             filename  \n",
       "0   s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "1   s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "2   s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "3   s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "4   s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "..                                                ...  \n",
       "59  s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "60  s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "61  s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "62  s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "63  s3://tensorflow/renforcement/rawdata/2012-01/Q...  \n",
       "\n",
       "[64 rows x 18 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "=========================================== TEST =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PartitionDate = [ d[:-3] for d in  os.listdir(H5_PATH)]\n",
    "random.shuffle(PartitionDate)\n",
    "print(PartitionDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partdate in PartitionDate[:1] :\n",
    "    DATA = []\n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        for k in [*f.keys()]:\n",
    "            DATA = f[k][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(DATA[0])\n",
    "df.columns = OTHER_COL+SCALER_COL\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(DATA[1])\n",
    "df.columns = OTHER_COL+SCALER_COL\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(DATA[0])\n",
    "df.columns = OTHER_COL+SCALER_COL\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(DATA[1])\n",
    "df.columns = OTHER_COL+SCALER_COL\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(DATA[2])\n",
    "df.columns = OTHER_COL+SCALER_COL\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(DATA[3])\n",
    "df.columns = SCALER_COL\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f77a7efb8cf15d18a0cd6bbc71a8985efbc57e2467f435a53ada42728ce0a69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
