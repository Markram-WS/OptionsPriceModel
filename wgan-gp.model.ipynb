{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/generative/wgan_gp/\n",
    "#https://keras.io/examples/generative/wgan-graphs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil,random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output,display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from src.wgangp.utils import Scaler\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== initialization ==================\n",
    "#best LR = 1e-4,EPOCHS = 10,\"discriminator_extra_steps\":1\n",
    "#best LR = 1e-4,EPOCHS = 15,\"discriminator_extra_steps\":1\n",
    "\n",
    "currentTM=dt.datetime.now().strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "PROJECT = \"wgangpModel\"\n",
    "LATENT_DIM = 16\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "PARQUET_PATH = './data/OptionsEOD_STG.parquet'\n",
    "\n",
    "UNIQUE_KEYS = ['QUOTE_DATE','SYMBOL','EXPIRE_DATE']\n",
    "SCALER_COL  = ['UNDERLYING_LAST','STRIKE','STRIKE_DISTANCE','INTRINSIC_VALUE','DTE','TOTAL_VOLUME','C_VEGA','P_VEGA',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK', 'P_VOLUME' ]\n",
    "\n",
    "MODEL_PATH = \"./models/\"\n",
    "H5_PATH = './data/OptTrainData/'\n",
    "STACK_DATA_SHAPE = np.empty((0,) + (16, len( SCALER_COL)  ) ) \n",
    "\n",
    "WANDB_LOG = True\n",
    "RESUME = False\n",
    "SUMMARY = True\n",
    "log_dir = f\"/app/logs/{PROJECT}/\"+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Scaler = joblib.load(SCALER_PATH )\n",
    "# Scaler_Price = joblib.load(SCALER_PRICE_PATH )\n",
    "\n",
    "#DISPLAY = ['map','summary',None]\n",
    "DISPLAY = 'summary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###===== load Scaler\n",
    "QUOTE_COL = ['C_BID',\t'C_ASK',  'P_BID',\t'P_ASK']\n",
    "VEGA_COL =  [\"C_VEGA\",\"P_VEGA\"] \n",
    "VOLUME_COL =  [\"TOTAL_VOLUME\",\"C_VOLUME\",\"P_VOLUME\"] \n",
    "\n",
    "SCALER_QUOTE_COL_INDEX = [i for i,v in enumerate(SCALER_COL) if v in QUOTE_COL]\n",
    "SCALER_VEGA_COL_INDEX = [i for i,v in enumerate(SCALER_COL) if v in VEGA_COL]\n",
    "SCALER_VOLUME_COL_INDEX = [i for i,v in enumerate(SCALER_COL) if v in VOLUME_COL]\n",
    "SCALER_OTHER_COL_INDEX = [i for i,v in enumerate(SCALER_COL) if v not in QUOTE_COL+VEGA_COL+VOLUME_COL ]\n",
    "\n",
    "SCALER = Scaler(\n",
    "    [v for v in SCALER_COL if v not in QUOTE_COL+VEGA_COL+VOLUME_COL ] + ['QUOTE','VEGA','VOLUME']\n",
    "    , path=\"./data/scaler/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##['UNDERLYING_LAST','STRIKE','STRIKE_DISTANCE','INTRINSIC_VALUE','DTE','TOTAL_VOLUME','C_VEGA','P_VEGA',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK', 'P_VOLUME' ]\n",
    "select_x = [i for i,c in  enumerate(SCALER_COL) if c in ['DTE','INTRINSIC_VALUE','C_VEGA','P_VEGA'] ]\n",
    "select_y = [i for i,c in enumerate(SCALER_COL) if c in ['C_BID','C_ASK',  'P_BID',\t'P_ASK'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "CONFIG = {\n",
    "          \"learning_rate\": LR,\n",
    "          \"epochs\": EPOCHS,\n",
    "          \"batch_size\": BATCH_SIZE,\n",
    "          \"architecture\": \"wgangp\",\n",
    "          \"dataset\": \"OptionsChaine\",\n",
    "          \"generator_dense_units\":[128,64,32],\n",
    "          \"generator_dropout_rate\":0.2,\n",
    "          \"discriminator_dense_units\":[32,64,128],\n",
    "          \"discriminator_dropout_rate\":0.2,\n",
    "          \"use_bias\":False,\n",
    "          \"use_dropout\":True,\n",
    "          \"use_bn\":True,\n",
    "          \"transform\":True,\n",
    "          \"discriminator_extra_steps\":1,\n",
    "          \"x_col\":select_x,\n",
    "          \"y_col\":select_y,\n",
    "          \"gp_weight\":(10.0,),\n",
    "          \"gp_cap\": 20\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "notes = f\"\"\"\n",
    "DenseLayer model\n",
    "additional_con_loss_p_1/c_1 * 0.01\n",
    "discriminator_extra_steps : {CONFIG['discriminator_extra_steps']}\n",
    "gp_weight:{CONFIG['gp_weight']}\n",
    "LR:{LR}\n",
    "gp cap : {CONFIG['gp_cap']}\n",
    "reduce_mean(ASK-BID) \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if WANDB_LOG :\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=PROJECT, \n",
    "                     name=currentTM, \n",
    "                     config=CONFIG,\n",
    "                     notes=notes\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wgangp.model import OptionChainGenerator\n",
    "from src.wgangp.layers import generatorDense as generator\n",
    "from src.wgangp.layers import discriminatorDense as discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = OptionChainGenerator(\n",
    "    discriminator = discriminator(\n",
    "            input_shape= (16,len(select_y) ), \n",
    "            dense_units = CONFIG[\"generator_dense_units\"], \n",
    "            dropout_rate= CONFIG[\"generator_dropout_rate\"],\n",
    "            use_bias=CONFIG[\"use_bias\"],\n",
    "            use_dropout=CONFIG[\"use_dropout\"],\n",
    "            use_bn=CONFIG[\"use_bn\"]\n",
    "           ), \n",
    "    generator = generator(\n",
    "            input_dim = (16,len(select_y) ),\n",
    "            output_dim = (16,len(select_x) ) ,\n",
    "            dense_units = CONFIG[\"discriminator_dense_units\"],\n",
    "            dropout_rate= CONFIG[\"discriminator_dropout_rate\"],\n",
    "            use_bias=CONFIG[\"use_bias\"],\n",
    "            use_dropout=CONFIG[\"use_dropout\"],\n",
    "            use_bn=CONFIG[\"use_bn\"]\n",
    "           ),\n",
    "    discriminator_extra_steps = CONFIG[\"discriminator_extra_steps\"],\n",
    "    output_col=[SCALER_COL[i] for i in select_y] ,\n",
    "    scaler = SCALER,\n",
    "    gp_weight=CONFIG[\"gp_weight\"],\n",
    "    gp_cap=CONFIG[\"gp_cap\"]\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    d_optimizer = tf.optimizers.Adam(\n",
    "    learning_rate=LR, beta_1=0.5, beta_2=0.9 , clipvalue=10.0\n",
    "    ),\n",
    "    g_optimizer = tf.optimizers.Adam(\n",
    "    learning_rate=LR, beta_1=0.5, beta_2=0.9 , clipvalue=10.0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## show model ######################\n",
    "if DISPLAY == 'map' :\n",
    "    from tensorflow.keras.utils import model_to_dot\n",
    "    from IPython.display import SVG, display\n",
    "    \n",
    "    def display_model(model, width=1024, height=512):\n",
    "        dot = model_to_dot(model, show_shapes=True, show_layer_names=True)\n",
    "        svg_data = dot.create(prog='dot', format='svg').decode(\"utf-8\")\n",
    "        svg_html = f'<div style=\"width:{width}px;height:{height}px;\">{svg_data}</div>'\n",
    "        display(HTML(svg_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage:\n",
    "## Display the generator model with reduced size\n",
    "if DISPLAY == 'map' :\n",
    "    display_model(model.generator, width, height=512)\n",
    "if DISPLAY == 'summary' :\n",
    "    model.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISPLAY == 'map' :\n",
    "    display_model(model.discriminator, width=2500, height=512)\n",
    "if DISPLAY == 'summary' :\n",
    "    model.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== loadmodel ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_path = MODEL_PATH+f'{PROJECT}'\n",
    "if not RESUME :\n",
    "    if os.path.exists(model_path) :\n",
    "        shutil.rmtree(model_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.generator.save(model_path+f'/'+f'generator.keras') \n",
    "    model.discriminator.save(model_path+f'/'+f'discriminator.keras') \n",
    "else:\n",
    "    model.generator = load_model(model_path+'/'+f'generator.keras') \n",
    "    model.discriminator = load_model(model_path+'/'+f'discriminator.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== train model ==================\n",
    "PartitionDate = [ d[:-3] for d in  os.listdir(H5_PATH)]\n",
    "random.shuffle(PartitionDate)\n",
    "#SET MODEL VAR\n",
    "STACK_DATA = STACK_DATA_SHAPE \n",
    "#INIT MODEL VAR\n",
    "STOP_MODEL = False\n",
    "\n",
    "#set PartitionDate[:] for limit range\n",
    "for partdate in PartitionDate[:] :\n",
    "    clear_output(wait=False)\n",
    "    DATA = []\n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        DATA = f[partdate][:]\n",
    "    data_shape = DATA.shape\n",
    "    ###transform\n",
    "    if CONFIG['transform'] :\n",
    "        #other index\n",
    "        for i, c in enumerate (SCALER_COL):\n",
    "            DATA[:,:,[i]] = SCALER.groupTransform(c,DATA[:,:,[i]].reshape(-1,1)\n",
    "                                                ,QUOTE_COL = QUOTE_COL\n",
    "                                               ,VEGA_COL = VEGA_COL\n",
    "                                               ,VOLUME_COL = VOLUME_COL\n",
    "                                                 ).reshape(-1, 16, 1)\n",
    "    DATA = np.vstack((DATA ,STACK_DATA))\n",
    "\n",
    "    if len(DATA) < 64 :\n",
    "        #stack data\n",
    "        STACK_DATA = np.vstack((STACK_DATA ,DATA))\n",
    "    else: \n",
    "        STACK_DATA = np.empty((0,) + data_shape[1:] )\n",
    "        X = DATA[:, :, select_x]  # เลือกข้อมูล select_x สำหรับ X\n",
    "        Y = DATA[:, :, select_y]  # เลือกข้อมูล select_y เสำหรับ Y\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        random.shuffle(PartitionDate)\n",
    "        tf.keras.backend.clear_session() \n",
    "        history = model.fit(x_train , y_train, epochs=CONFIG['epochs'], batch_size=BATCH_SIZE, validation_data=(x_val, y_val) ,callbacks=[tensorboard_callback])\n",
    "        if  np.isnan(  np.average( history.history['generator_loss'] )  ) or np.isnan(  np.average( history.history['discriminator_loss'] )  ):\n",
    "            STOP_MODEL = True \n",
    "    \n",
    "        if WANDB_LOG :\n",
    "            LogKeys = history.history.keys()\n",
    "            LogVal={}\n",
    "            for k in LogKeys:  \n",
    "                LogVal[k] = np.average(  history.history[k] )\n",
    "            wandb.log(LogVal, commit=True)\n",
    "    if STOP_MODEL :\n",
    "        break\n",
    "    \n",
    "            \n",
    "    model.generator.save(model_path+f'/'+f'generator.keras') \n",
    "    model.discriminator.save(model_path+f'/'+f'discriminator.keras') \n",
    "if WANDB_LOG : wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`====================== GET ZERO Val. ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_ZERO = np.full((1,16, 14), 1.00e-08)\n",
    "for i, c in enumerate (SCALER_COL):\n",
    "    D_ZERO[:,:,[i]] = SCALER.groupTransform(c,D_ZERO[:,:,[i]].reshape(-1,1)\n",
    "                                           ,QUOTE_COL = QUOTE_COL\n",
    "                                           ,VEGA_COL = VEGA_COL\n",
    "                                           ,VOLUME_COL = VOLUME_COL\n",
    "                                           ).reshape(-1, 16, 1)\n",
    "a=pd.DataFrame(  \n",
    "    D_ZERO.reshape(16, 14)\n",
    ",columns=SCALER_COL)\n",
    "a[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "====================== CheckData ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartitionDate = [ d[:-3] for d in  os.listdir(H5_PATH)]\n",
    "random.shuffle(PartitionDate)\n",
    "for partdate in PartitionDate[:1] :\n",
    "    DATA = []\n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        DATA = f[partdate][:]\n",
    "        DATA_ORIGINAL = f[partdate][:]\n",
    "    data_shape = DATA.shape\n",
    "    print(f\"CONFIG['transform'] : {CONFIG['transform']}\")\n",
    "    for i, c in enumerate (SCALER_COL):\n",
    "        DATA[:,:,[i]] = SCALER.groupTransform(c,DATA[:,:,[i]].reshape(-1,1)\n",
    "                                               ,QUOTE_COL = QUOTE_COL\n",
    "                                               ,VEGA_COL = VEGA_COL\n",
    "                                               ,VOLUME_COL = VOLUME_COL\n",
    "                                               ).reshape(-1, 16, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_x = [SCALER_COL[i] for i in select_x]\n",
    "col_y = [SCALER_COL[i] for i in select_y]\n",
    "DATA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA[:,:,SCALER_QUOTE_COL_INDEX][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ORIGINAL[:,:,SCALER_QUOTE_COL_INDEX][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in SCALER_QUOTE_COL_INDEX:\n",
    "    DATA[:,:,i] = SCALER()['QUOTE'].inverse_transform(DATA[:,:,i] )\n",
    "DATA[:,:,SCALER_QUOTE_COL_INDEX] [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================== predict ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartitionDate = [ d[:-3] for d in  os.listdir(H5_PATH)]\n",
    "random.shuffle(PartitionDate)\n",
    "for partdate in PartitionDate[:1] :\n",
    "    DATA = []\n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        DATA = f[partdate][:]\n",
    "        DATA_ORIGINAL = f[partdate][:]\n",
    "    data_shape = DATA.shape\n",
    "    print(f\"CONFIG['transform'] : {CONFIG['transform']}\")\n",
    "    for i, c in enumerate (SCALER_COL):\n",
    "        DATA[:,:,[i]] = SCALER.groupTransform(c,DATA[:,:,[i]].reshape(-1,1)\n",
    "                                               ,QUOTE_COL = QUOTE_COL\n",
    "                                               ,VEGA_COL = VEGA_COL\n",
    "                                               ,VOLUME_COL = VOLUME_COL\n",
    "                                               ).reshape(-1, 16, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_x = [SCALER_COL[i] for i in select_x]\n",
    "col_y = [SCALER_COL[i] for i in select_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DATA[:, :, select_x][:]\n",
    "Y_real = DATA[:, :, select_y][:]\n",
    "#===========\n",
    "# X = x_train\n",
    "# Y_real = y_train\n",
    "# #===========\n",
    "dfX = pd.DataFrame(\n",
    "    X[:1].reshape(16, len(select_x)), \n",
    "    columns=col_x)\n",
    "#print(dfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genVal = model.generator(X) \n",
    "disVal = model.discriminator(genVal, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "========================== _LOSS_ANAYSIS ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-tf.reduce_mean(disVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "            [SCALER_COL[i] for i in select_y][i]: genVal[:, :, i]\n",
    "            for i in range(len([SCALER_COL[i] for i in select_y]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_con_loss_c_1 = tf.reduce_mean(\n",
    "            tf.maximum(data_dict[\"C_ASK\"] - data_dict[\"C_BID\"], 0.0)\n",
    "        )\n",
    "additional_con_loss_c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_con_loss_c_2 = []\n",
    "for c, v in [(\"C_BID\", -4.999975e-14), (\"C_ASK\", -4.999975e-14)]:\n",
    "    # tf mark with zero\n",
    "    mask = tf.greater(data_dict[c], v)\n",
    "    filtered_tensor = tf.boolean_mask(data_dict[c], mask)\n",
    "    filtered_tensor_roll = tf.roll(filtered_tensor, shift=-1, axis=0)\n",
    "    difference = tf.maximum(\n",
    "        filtered_tensor_roll[:-1] - filtered_tensor[:-1], 0.0\n",
    "    )\n",
    "    additional_con_loss_c_2.append(tf.reduce_sum(difference))\n",
    "additional_con_loss_c_2 = tf.reduce_mean(additional_con_loss_c_2)\n",
    "additional_con_loss_c_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_con_loss_p_2 = []\n",
    "for p, v in [(\"P_BID\", -4.999975e-14), (\"P_ASK\", -4.999975e-14)]:\n",
    "    # tf mark with zero\n",
    "    mask = tf.greater(data_dict[p], v)\n",
    "    filtered_tensor = tf.boolean_mask(data_dict[p], mask)\n",
    "    filtered_tensor_roll = tf.roll(filtered_tensor, shift=-1, axis=0)\n",
    "    difference = tf.maximum(\n",
    "        filtered_tensor[:-1] - filtered_tensor_roll[:-1], 0.0\n",
    "    )\n",
    "    additional_con_loss_p_2.append(tf.reduce_sum(difference))\n",
    "additional_con_loss_p_2 = tf.reduce_mean(additional_con_loss_p_2)\n",
    "additional_con_loss_p_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[p][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.greater(data_dict[p][0], -4.999975e-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfY = pd.DataFrame(\n",
    "    genVal.numpy()[:1].reshape(16, len(select_y)), \n",
    "    columns=col_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npgenVal = genVal.numpy()\n",
    "for i in range(genVal.shape[-1]):\n",
    "    npgenVal[:,:,i] = SCALER()['QUOTE'].inverse_transform(npgenVal[:,:,i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(npgenVal[0],columns=col_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_invT = Y_real\n",
    "for i in range(Y_invT.shape[-1]):\n",
    "    Y_invT[:,:,i] = SCALER()['QUOTE'].inverse_transform(Y_real[:,:,i] )\n",
    "pd.DataFrame(Y_invT[0],columns=col_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(DATA_ORIGINAL[:,:,select_y][0],columns=col_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMDF = pd.concat([dfX, dfY],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = pd.DataFrame([])\n",
    "rm_col = []\n",
    "for i in SCALER_COL:\n",
    "    if i in SUMDF.columns:\n",
    "         resultDF[i] = SUMDF[i]\n",
    "    else:\n",
    "        rm_col += [i]\n",
    "        resultDF[i] = [1e-8]*16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['transform'] :\n",
    "    decode_transformed=Scaler.inverse_transform(\n",
    "        resultDF\n",
    "    )\n",
    "    decode_transformed=pd.DataFrame(  \n",
    "        decode_transformed\n",
    "    ,columns=SCALER_COL).drop(columns=rm_col)\n",
    "    decode_transformed.loc[decode_transformed['DTE'] < 1e-8] = 0\n",
    "    decode_transformed[col_y] = decode_transformed[col_y].round(2)\n",
    "decode_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdVal = tf.constant( resultDF[['C_BID','C_ASK']].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.greater(tdVal[:,0], -0.502343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tensor = tf.boolean_mask(tdVal[:,0], mask)\n",
    "filtered_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tensor_roll = tf.roll(filtered_tensor, shift=-1, axis=0)\n",
    "filtered_tensor_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = tf.maximum(\n",
    "                filtered_tensor_roll[:-1] - filtered_tensor[:-1], 0.0\n",
    "            )\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================= real data =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['transform'] :\n",
    "    realdecode_transformed=Scaler.inverse_transform(\n",
    "        DATA[0]\n",
    "    )\n",
    "    realdecode_transformed=pd.DataFrame(  \n",
    "        realdecode_transformed\n",
    "    ,columns=SCALER_COL).drop(columns=rm_col)\n",
    "    realdecode_transformed.loc[realdecode_transformed['DTE'] <= 1e-8] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdecode_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9.999994e-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================= _compute_loss =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated_data = [c_bid, c_ask, c_volume, p_bid, p_ask, p_volume]\n",
    "colList = [\"c_bid\", \"c_ask\", \"c_volume\", \"p_bid\", \"p_ask\", \"p_volume\"]\n",
    "generated_data = decode_data[3:]\n",
    "z_mean    = z_mean\n",
    "z_log_var = log_var\n",
    "Y_real    = DATA[:, :, 3:][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  col,genData in zip(colList,generated_data):\n",
    "    print( colList.index(col),col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract_genData = genData - tf.cast(tf.expand_dims(Y_real[:, :, colList.index(col)], axis=-1)\n",
    "        , tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_values_total = []\n",
    "reconstruction_values_total.append( tf.reduce_mean( tf.square(subtract_genData)   ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_var = tf.clip_by_value(log_var, -1.0, 1.0)\n",
    "kl_loss = -0.5 * tf.reduce_sum(1 + log_var - tf.square(z_mean) - tf.exp(log_var), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(reconstruction_values_total + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "========== kiras vae origi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.concat(decode_data, axis=-1).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real[0] -  tf.concat(decode_data, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loss = tf.reduce_mean(\n",
    "    tf.reduce_sum(\n",
    "        tf.keras.losses.mean_squared_error(Y_real, tf.concat(decode_data, axis=-1)),\n",
    "        axis=(1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_loss = tf.reduce_mean(\n",
    "#     tf.reduce_sum(\n",
    "#         keras.losses.categorical_crossentropy(features_real, features_gen),\n",
    "#         axis=(1),\n",
    "#     )\n",
    "# )\n",
    "# kl_loss = -0.5 * tf.reduce_sum(\n",
    "#     1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1\n",
    "# )\n",
    "# kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "# property_loss = tf.reduce_mean(\n",
    "#     keras.losses.binary_crossentropy(qed_true, qed_pred)\n",
    "# )\n",
    "\n",
    "# graph_loss = self._gradient_penalty(graph_real, graph_generated)\n",
    "\n",
    "# return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================= inverse_transform ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 0\n",
    "decode_data = [tf.zeros([1, 32, 1])]*3 + decode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_decode = Scaler.inverse_transform(\n",
    "    np.array([d.numpy().reshape(-1) for d in decode_data]).transpose()\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    invert_decode[:,3:], \n",
    "    columns=SCALER_COL[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.reshape(-1,data_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix = np.array([\n",
    "[1,2,3,],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tf.convert_to_tensor(matrix)\n",
    "    \n",
    "# Shift matrix to compare each row with the next row\n",
    "matrix_next = tf.roll(matrix, shift=-1, axis=0)\n",
    "\n",
    "# Ignore the last row for comparison as it rolls over to the first row\n",
    "matrix = matrix[:-1]\n",
    "matrix_next = matrix_next[:-1]\n",
    "\n",
    "# Compute the difference between each row and the next row\n",
    "difference = matrix_next - matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps( \"\"\" WHERE \n",
    "Count_Date < LEFT( DATEADD( month,2,CAST('$timestamp' AS date) ),7)+'-01')\n",
    "AND Count_Date > LEFT( DATEADD( year,-2,CAST('$timestamp' AS date) ),4)+'-01-01' \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f77a7efb8cf15d18a0cd6bbc71a8985efbc57e2467f435a53ada42728ce0a69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
