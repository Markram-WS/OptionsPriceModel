{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/generative/wgan_gp/\n",
    "#https://keras.io/examples/generative/wgan-graphs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil,random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output,display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== initialization ==================\n",
    "#best LR = 1e-4,EPOCHS = 10,\"discriminator_extra_steps\":1\n",
    "#best LR = 1e-4,EPOCHS = 15,\"discriminator_extra_steps\":1\n",
    "\n",
    "currentTM=dt.datetime.now().strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "PROJECT = \"wgangpModel\"\n",
    "LATENT_DIM = 16\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "PARQUET_PATH = './data/OptionsEOD_STG.parquet'\n",
    "SCALER_PATH = './data/scaler/scaler.gz'\n",
    "UNIQUE_KEYS = ['QUOTE_DATE','SYMBOL','EXPIRE_DATE']\n",
    "SCALER_COL  = ['UNDERLYING_LAST','STRIKE','STRIKE_DISTANCE','INTRINSIC_VALUE','DTE','TOTAL_VOLUME','C_VEGA','P_VEGA',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK', 'P_VOLUME' ]\n",
    "MODEL_PATH = \"./models/\"\n",
    "H5_PATH = './data/OptTrainData/'\n",
    "STACK_DATA_SHAPE = np.empty((0,) + (16, len( SCALER_COL)  ) ) \n",
    "\n",
    "WANDB_LOG = True\n",
    "RESUME = False\n",
    "SUMMARY = True\n",
    "log_dir = f\"/app/logs/{PROJECT}/\"+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "Scaler = joblib.load(SCALER_PATH )\n",
    "\n",
    "#DISPLAY = ['map','summary',None]\n",
    "DISPLAY = 'summary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##['UNDERLYING_LAST','STRIKE','STRIKE_DISTANCE','INTRINSIC_VALUE','DTE','TOTAL_VOLUME','C_VEGA','P_VEGA',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK', 'P_VOLUME' ]\n",
    "select_x = [i for i,c in  enumerate(SCALER_COL) if c in ['DTE','INTRINSIC_VALUE','C_VEGA','P_VEGA'] ]\n",
    "select_y = [i for i,c in enumerate(SCALER_COL) if c in ['C_BID','C_ASK',  'P_BID',\t'P_ASK'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = f\"\"\"\n",
    "test Run use_bias set false , no tranfrom\n",
    "\"\"\"\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "CONFIG = {\n",
    "          \"learning_rate\": LR,\n",
    "          \"epochs\": EPOCHS,\n",
    "          \"batch_size\": BATCH_SIZE,\n",
    "          \"architecture\": \"wgangp\",\n",
    "          \"dataset\": \"OptionsChaine\",\n",
    "          \"generator_dense_units\":[128,64,32],\n",
    "          \"generator_dropout_rate\":0.2,\n",
    "          \"discriminator_dense_units\":[32,64,128],\n",
    "          \"discriminator_dropout_rate\":0.2,\n",
    "          \"use_bias\":False,\n",
    "          \"use_dropout\":True,\n",
    "          \"use_bn\":True,\n",
    "          \"transform\":True,\n",
    "          \"discriminator_extra_steps\":1,\n",
    "          \"x_col\":select_x,\n",
    "          \"y_col\":select_y,\n",
    "           }\n",
    "\n",
    "if WANDB_LOG :\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=PROJECT, \n",
    "                     name=currentTM, \n",
    "                     config=CONFIG,\n",
    "                     notes=notes\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wgangp.model import OptionChainGenerator\n",
    "from src.wgangp.layer import generator, discriminator\n",
    "\n",
    "model = OptionChainGenerator(\n",
    "    discriminator = discriminator(\n",
    "            input_shape= (16,len(select_y) ), \n",
    "            dense_units = CONFIG[\"generator_dense_units\"], \n",
    "            dropout_rate= CONFIG[\"generator_dropout_rate\"],\n",
    "            use_bias=CONFIG[\"use_bias\"],\n",
    "            use_dropout=CONFIG[\"use_dropout\"],\n",
    "            use_bn=CONFIG[\"use_bn\"]\n",
    "           ), \n",
    "    generator = generator(\n",
    "            input_dim = (16,len(select_y) ),\n",
    "            output_dim = (16,len(select_x) ) ,\n",
    "            dense_units = CONFIG[\"discriminator_dense_units\"],\n",
    "            dropout_rate= CONFIG[\"discriminator_dropout_rate\"],\n",
    "            use_bias=CONFIG[\"use_bias\"],\n",
    "            use_dropout=CONFIG[\"use_dropout\"],\n",
    "            use_bn=CONFIG[\"use_bn\"]\n",
    "           ),\n",
    "    discriminator_extra_steps = CONFIG[\"discriminator_extra_steps\"],\n",
    "    output_col=[SCALER_COL[i] for i in select_y] ,\n",
    "    scaler = Scaler\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    d_optimizer = tf.optimizers.Adam(\n",
    "    learning_rate=LR, beta_1=0.5, beta_2=0.9\n",
    "    ),\n",
    "    g_optimizer = tf.optimizers.Adam(\n",
    "    learning_rate=LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## show model ######################\n",
    "if DISPLAY == 'map' :\n",
    "    from tensorflow.keras.utils import model_to_dot\n",
    "    from IPython.display import SVG, display\n",
    "    \n",
    "    def display_model(model, width=1024, height=512):\n",
    "        dot = model_to_dot(model, show_shapes=True, show_layer_names=True)\n",
    "        svg_data = dot.create(prog='dot', format='svg').decode(\"utf-8\")\n",
    "        svg_html = f'<div style=\"width:{width}px;height:{height}px;\">{svg_data}</div>'\n",
    "        display(HTML(svg_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage:\n",
    "## Display the generator model with reduced size\n",
    "if DISPLAY == 'map' :\n",
    "    display_model(model.generator, width, height=512)\n",
    "if DISPLAY == 'summary' :\n",
    "    model.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISPLAY == 'map' :\n",
    "    display_model(model.discriminator, width=2500, height=512)\n",
    "if DISPLAY == 'summary' :\n",
    "    model.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== loadmodel ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_path = MODEL_PATH+f'{PROJECT}'\n",
    "if not RESUME :\n",
    "    if os.path.exists(model_path) :\n",
    "        shutil.rmtree(model_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.generator.save(model_path+f'/'+f'generator.keras') \n",
    "    model.discriminator.save(model_path+f'/'+f'discriminator.keras') \n",
    "else:\n",
    "    model.generator = load_model(model_path+'/'+f'generator.keras') \n",
    "    model.discriminator = load_model(model_path+'/'+f'discriminator.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== train model ==================\n",
    "PartitionDate = [ d[:-3] for d in  os.listdir(H5_PATH)]\n",
    "random.shuffle(PartitionDate)\n",
    "#SET MODEL VAR\n",
    "STACK_DATA = STACK_DATA_SHAPE \n",
    "#INIT MODEL VAR\n",
    "STOP_MODEL = False\n",
    "\n",
    "#set PartitionDate[:] for limit range\n",
    "for partdate in PartitionDate[:] :\n",
    "    clear_output(wait=False)\n",
    "    DATA = []\n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        DATA = f[partdate][:]\n",
    "    data_shape = DATA.shape\n",
    "    ###transform\n",
    "    if CONFIG['transform'] :\n",
    "        DATA = Scaler.transform(DATA.reshape(-1,data_shape[-1]))\n",
    "        DATA = DATA.reshape(data_shape)\n",
    "        #! เช็ตปัญหาการ แปลง shape\n",
    "        #inv.tran data ไม่ได้ค่าเดิม\n",
    "    DATA = np.vstack((DATA ,STACK_DATA))\n",
    "\n",
    "    if len(DATA) < 64 :\n",
    "        #stack data\n",
    "        STACK_DATA = np.vstack((STACK_DATA ,DATA))\n",
    "    else: \n",
    "        STACK_DATA = np.empty((0,) + data_shape[1:] )\n",
    "        X = DATA[:, :, select_x]  # เลือกข้อมูล select_x สำหรับ X\n",
    "        Y = DATA[:, :, select_y]  # เลือกข้อมูล select_y เสำหรับ Y\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        random.shuffle(PartitionDate)\n",
    "        tf.keras.backend.clear_session() \n",
    "        history = model.fit(x_train , y_train, epochs=CONFIG['epochs'], batch_size=BATCH_SIZE, validation_data=(x_val, y_val) ,callbacks=[tensorboard_callback])\n",
    "        if  np.isnan(  np.average( history.history['generator_loss'] )  ) or np.isnan(  np.average( history.history['discriminator_loss'] )  ):\n",
    "            STOP_MODEL = True \n",
    "    \n",
    "        if WANDB_LOG :\n",
    "            LogKeys = history.history.keys()\n",
    "            LogVal={}\n",
    "            for k in LogKeys:  \n",
    "                LogVal[k] = np.average(  history.history[k] )\n",
    "            wandb.log(LogVal, commit=True)\n",
    "    if STOP_MODEL :\n",
    "        break\n",
    "    \n",
    "            \n",
    "    model.generator.save(model_path+f'/'+f'generator.keras') \n",
    "    model.discriminator.save(model_path+f'/'+f'discriminator.keras') \n",
    "if WANDB_LOG : wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ชช-=-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_num = 1\n",
    "partd = '2011-12'\n",
    "with h5py.File(H5_PATH+partd+\".h5\", 'r') as f:\n",
    "    d = f[partd][:]\n",
    "#show sample data\n",
    "d=Scaler.inverse_transform(\n",
    "        d[15]\n",
    "    )\n",
    "d=pd.DataFrame(  \n",
    "    d\n",
    ",columns=SCALER_COL)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show zero data\n",
    "d = Scaler.transform(\n",
    "        np.zeros((16, 14))\n",
    "    )\n",
    "d=pd.DataFrame(  \n",
    "    d\n",
    ",columns=SCALER_COL)\n",
    "d[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================== predict ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartitionDate = [ d[:-3] for d in  os.listdir(H5_PATH)]\n",
    "random.shuffle(PartitionDate)\n",
    "for partdate in PartitionDate[:1] :\n",
    "    DATA = []\n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        DATA = f[partdate][:]\n",
    "    data_shape = DATA.shape\n",
    "    print(f\"CONFIG['transform'] : {CONFIG['transform']}\")\n",
    "    if CONFIG['transform'] or True :\n",
    "        DATA = Scaler.transform(DATA.reshape(-1,data_shape[-1]))\n",
    "        DATA = DATA.reshape(data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_x = [SCALER_COL[i] for i in select_x]\n",
    "col_y = [SCALER_COL[i] for i in select_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DATA[:, :, select_x][:]\n",
    "Y_real = DATA[:, :, select_y][:]\n",
    "#===========\n",
    "# X = x_train\n",
    "# Y_real = y_train\n",
    "# #===========\n",
    "dfX = pd.DataFrame(\n",
    "    X[:1].reshape(16, len(select_x)), \n",
    "    columns=col_x)\n",
    "#print(dfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genVal = model.generator(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfY = pd.DataFrame(\n",
    "    genVal.numpy()[:1].reshape(16, len(select_y)), \n",
    "    columns=col_y)\n",
    "#print(dfY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMDF = pd.concat([dfX, dfY],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = pd.DataFrame([])\n",
    "rm_col = []\n",
    "for i in SCALER_COL:\n",
    "    if i in SUMDF.columns:\n",
    "         resultDF[i] = SUMDF[i]\n",
    "    else:\n",
    "        rm_col += [i]\n",
    "        resultDF[i] = [1e-8]*16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['transform'] :\n",
    "    decode_transformed=Scaler.inverse_transform(\n",
    "        resultDF\n",
    "    )\n",
    "    decode_transformed=pd.DataFrame(  \n",
    "        decode_transformed\n",
    "    ,columns=SCALER_COL).drop(columns=rm_col)\n",
    "    decode_transformed.loc[decode_transformed['DTE'] < 1e-8] = 0\n",
    "    decode_transformed[col_y] = decode_transformed[col_y].round(2)\n",
    "decode_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdVal = tf.constant( resultDF[['C_BID','C_ASK']].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.greater(tdVal[:,0], -0.502343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tensor = tf.boolean_mask(tdVal[:,0], mask)\n",
    "filtered_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tensor_roll = tf.roll(filtered_tensor, shift=-1, axis=0)\n",
    "filtered_tensor_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = tf.maximum(\n",
    "                filtered_tensor_roll[:-1] - filtered_tensor[:-1], 0.0\n",
    "            )\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================= real data =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['transform'] :\n",
    "    realdecode_transformed=Scaler.inverse_transform(\n",
    "        DATA[0]\n",
    "    )\n",
    "    realdecode_transformed=pd.DataFrame(  \n",
    "        realdecode_transformed\n",
    "    ,columns=SCALER_COL).drop(columns=rm_col)\n",
    "    realdecode_transformed.loc[realdecode_transformed['DTE'] <= 1e-8] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdecode_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9.999994e-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================= _compute_loss =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated_data = [c_bid, c_ask, c_volume, p_bid, p_ask, p_volume]\n",
    "colList = [\"c_bid\", \"c_ask\", \"c_volume\", \"p_bid\", \"p_ask\", \"p_volume\"]\n",
    "generated_data = decode_data[3:]\n",
    "z_mean    = z_mean\n",
    "z_log_var = log_var\n",
    "Y_real    = DATA[:, :, 3:][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  col,genData in zip(colList,generated_data):\n",
    "    print( colList.index(col),col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract_genData = genData - tf.cast(tf.expand_dims(Y_real[:, :, colList.index(col)], axis=-1)\n",
    "        , tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_values_total = []\n",
    "reconstruction_values_total.append( tf.reduce_mean( tf.square(subtract_genData)   ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_var = tf.clip_by_value(log_var, -1.0, 1.0)\n",
    "kl_loss = -0.5 * tf.reduce_sum(1 + log_var - tf.square(z_mean) - tf.exp(log_var), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(reconstruction_values_total + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "========== kiras vae origi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.concat(decode_data, axis=-1).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real[0] -  tf.concat(decode_data, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loss = tf.reduce_mean(\n",
    "    tf.reduce_sum(\n",
    "        tf.keras.losses.mean_squared_error(Y_real, tf.concat(decode_data, axis=-1)),\n",
    "        axis=(1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_loss = tf.reduce_mean(\n",
    "#     tf.reduce_sum(\n",
    "#         keras.losses.categorical_crossentropy(features_real, features_gen),\n",
    "#         axis=(1),\n",
    "#     )\n",
    "# )\n",
    "# kl_loss = -0.5 * tf.reduce_sum(\n",
    "#     1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1\n",
    "# )\n",
    "# kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "# property_loss = tf.reduce_mean(\n",
    "#     keras.losses.binary_crossentropy(qed_true, qed_pred)\n",
    "# )\n",
    "\n",
    "# graph_loss = self._gradient_penalty(graph_real, graph_generated)\n",
    "\n",
    "# return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================= inverse_transform ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 0\n",
    "decode_data = [tf.zeros([1, 32, 1])]*3 + decode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_decode = Scaler.inverse_transform(\n",
    "    np.array([d.numpy().reshape(-1) for d in decode_data]).transpose()\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    invert_decode[:,3:], \n",
    "    columns=SCALER_COL[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.reshape(-1,data_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix = np.array([\n",
    "[1,2,3,],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tf.convert_to_tensor(matrix)\n",
    "    \n",
    "# Shift matrix to compare each row with the next row\n",
    "matrix_next = tf.roll(matrix, shift=-1, axis=0)\n",
    "\n",
    "# Ignore the last row for comparison as it rolls over to the first row\n",
    "matrix = matrix[:-1]\n",
    "matrix_next = matrix_next[:-1]\n",
    "\n",
    "# Compute the difference between each row and the next row\n",
    "difference = matrix_next - matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps( \"\"\" WHERE \n",
    "Count_Date < LEFT( DATEADD( month,2,CAST('$timestamp' AS date) ),7)+'-01')\n",
    "AND Count_Date > LEFT( DATEADD( year,-2,CAST('$timestamp' AS date) ),4)+'-01-01' \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f77a7efb8cf15d18a0cd6bbc71a8985efbc57e2467f435a53ada42728ce0a69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
